{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#Parameters\n",
    "movie_path = \"/Users/devinwilson/Documents/lab_docs/data/INPUTS/test.tif\"\n",
    "output_directory = \"/Users/devinwilson/Documents/lab_docs/data/OUTPUTS/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create movie.tif folder within output directory\n",
    "import os\n",
    "movie_name = os.path.splitext(os.path.basename(movie_path))[0]\n",
    "output_filename = os.path.join(output_directory, movie_name)\n",
    "if not os.path.exists(output_filename):\n",
    "    os.makedirs(output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h1 style=\"margin-top: 0;\">Pipeline info and setup</h1>\n",
    "\n",
    "This pipeline is a modified version of the CNMF demo pipeline to function as a replacement for the LC_Pro imageJ plugin. Much of the information provided by the demo pipeline on how CaImAn works has been left in.\n",
    "\n",
    "If you need to alter the program or want to know more about a given function, you can find its help menu by typing a question mark after it where the dependencies would normally be defined. For example, if I wanted to use cnmf_fit.estimates as a part of a new portion of the pipeline, I could run an otherwise empty cell with 'cnmf_fit.estimates?' to find out what dependencies it expects.\n",
    "\n",
    "\n",
    "To run a single cell, use the play button on the top of the toolbar, or press shift+enter.\n",
    "To run the entire pipeline (all cells sequentially), use the fast-forward button in the toolbar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note: The CNMF algorithm is best for data with relatively low background noise, like most two-photon data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline executes the following analysis steps:\n",
    "\n",
    "![Full CNMF Workflow](images/full_cnmf_workflow.jpg)\n",
    "\n",
    "1) Apply the nonrigid motion correction (NoRMCorre) algorithm for motion correction.\n",
    "2) Apply the constrained nonnegative matrix factorization (CNMF) source separation algorithm to extract initial estimates of neuronal spatial footprints and calcium traces.  \n",
    "3) Apply quality control metrics to evaluate the initial estimates, and narrow down to the final set of estimates.\n",
    "\n",
    "In addition to the above core analysis steps, the pipeline also extracts $\\Delta F/F$ for the calcium traces, and makes use of multiple visualization tools to display the data at each step. This is helpful for assessing wether the settings you define in the \"set general parameters\" are adequate or need to be adjusted.\n",
    "\n",
    "The CNMF algorithm is best for data with relatively low background noise, like most two-photon data and *some* one photon data (e.g., certain light sheet data). For a demo analysis pipeline of a one-photon microendoscopic data set see `demo_pipeline_cnmfE.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h2 style=\"margin-top: 0;\">Getting help</h2>\n",
    "    More detailed background information about CNMF can be found in the <a href=\"https://github.com/flatironinstitute/CaImAn/blob/main/docs/source/Getting_Started.rst\">github 'getting started' page</a> and <a href=\"https://pubmed.ncbi.nlm.nih.gov/30652683/\">the Caiman paper</a>. If you have specific questions about this demo, or the underlying algorithms, you can ask questions at <a href=\"https://github.com/flatironinstitute/CaImAn/discussions\">GitHub Discussions</a>. If you find a bug or you have a feature request, you can <a href=\"https://github.com/flatironinstitute/CaImAn/issues\">open an issue at the Caiman Github repo</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bokeh.plotting as bpl\n",
    "import cv2\n",
    "import datetime\n",
    "import glob\n",
    "import holoviews as hv\n",
    "from IPython import get_ipython\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import psutil\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    cv2.setNumThreads(0)\n",
    "except():\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    if __IPYTHON__:\n",
    "        get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "        get_ipython().run_line_magic('autoreload', '2')\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "import caiman as cm\n",
    "from caiman.motion_correction import MotionCorrect\n",
    "from caiman.source_extraction.cnmf import cnmf, params\n",
    "from caiman.utils.utils import download_demo\n",
    "from caiman.utils.visualization import plot_contours, nb_view_patches, nb_plot_contour\n",
    "from caiman.utils.visualization import view_quilt\n",
    "\n",
    "bpl.output_notebook()\n",
    "hv.notebook_extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "logging.basicConfig(format=\"{asctime} - {levelname} - [{filename} {funcName}() {lineno}] - pid {process} - {message}\",\n",
    "                    filename=None, \n",
    "                    level=logging.WARNING, style=\"{\") #logging level can be DEBUG, INFO, WARNING, ERROR, CRITICAL\n",
    "\n",
    "# set env variables \n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize raw data\n",
    "Caiman has a built-in movie class for movie-viewing (documentation [here](https://caiman.readthedocs.io/en/latest/Handling_Movies.html)). Once you have loaded a movie using `cm.load()`, you can view it using `movie.play()`. The `play()` function has multiple parameters you can use to adjust the appearance of the movie: \n",
    "\n",
    "    gain: brightness \n",
    "    fr:  frame rate\n",
    "    magnification: scale the size of the display  \n",
    "    qmax, q_min: percentile for setting vmax, vmin -- below vmin is set to min, above vmax is set to max\n",
    "    plot_text (Bool): show the frame number\n",
    "    do_loop (Bool): whether to loop the video \n",
    "    \n",
    "The movie object also has a `resize()` method, which we use in the following to downsample the movie before playing.\n",
    "\n",
    "Playing the movie uses the `OpenCV` library, so the following cell runs a blocking function (a function that blocks execution of all other code until it is stopped): it will open a separate window that doesn't run in Jupyter. You will need to press `q` on that window to close it (or just wait until it is finished running). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# press q to close\n",
    "movie_orig = cm.load(movie_path) \n",
    "downsampling_ratio = 0.2  # subsample 5x\n",
    "movie_orig.resize(fz=downsampling_ratio).play(gain=1.3,\n",
    "                                              q_max=99.5, \n",
    "                                              fr=30,\n",
    "                                              plot_text=True,\n",
    "                                              magnification=2,\n",
    "                                              do_loop=False,\n",
    "                                              backend='opencv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_projection_orig = np.max(movie_orig, axis=0)\n",
    "correlation_image_orig = cm.local_correlations(movie_orig, swap_dim=False)\n",
    "correlation_image_orig[np.isnan(correlation_image_orig)] = 0 # get rid of NaNs, if they exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax_max, ax_corr) = plt.subplots(1,2,figsize=(6,3))\n",
    "ax_max.imshow(max_projection_orig, \n",
    "              cmap='gray',\n",
    "              vmin=np.percentile(np.ravel(max_projection_orig),50), \n",
    "              vmax=np.percentile(np.ravel(max_projection_orig),99.5));\n",
    "ax_max.set_title(\"Max Projection Orig\", fontsize=12);\n",
    "\n",
    "ax_corr.imshow(correlation_image_orig, \n",
    "               cmap='gray', \n",
    "               vmin=np.percentile(np.ravel(correlation_image_orig),50), \n",
    "               vmax=np.percentile(np.ravel(correlation_image_orig),99.5));\n",
    "ax_corr.set_title('Correlation Image Orig', fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" markdown=\"1\">\n",
    "    <h1 style=\"margin-top: 0;\">Set initial parameters</h1>\n",
    "In general in Caiman, estimators are first initialized with a set of parameters, and then they are fit against actual data in a separate step. In this section, we'll define a `parameters` object that will subsequently be used to initialize our different estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key parameters for CNMF\n",
    "\n",
    "`rf (int)`: *patch half-width*\n",
    "\n",
    "> `rf` ('receptive field') is the half width of patches in pixels (the actual patch width is `2*rf + 1`). See previous image for a representation of how the field of view is split up into patches for parallel procesing. `rf` should be *at least* 3-4 times larger than the observed neuron diameter. The larger the patch size, the less parallelization will be used by Caiman. If `rf` is set to `None`, then CNMF will be run on the entire field of view. \n",
    "\n",
    "`stride (int)`: *patch overlap*\n",
    "\n",
    "> `stride` is the overlap between patches in pixels (the actual overlap is `stride + 1`). This should be at least the diameter of a neuron. The larger the overlap, the greater the computational load, but the results will be more accurate when stitching together results from different patches. \n",
    "\n",
    "`K (int)`: *components per patch*\n",
    "\n",
    "> `K` is the expected number of components per patch. You should adapt this to the density of components in your data, and the current `rf` parameter. We suggest you pick `K` based on the more dense patches in your movie so you don't miss neurons (we want to avoid false negatives).\n",
    "\n",
    "`gSig (int, int)`: *half-width of neurons*\n",
    "\n",
    "> `gSig` is roughly the half-width of neurons in your movie in pixels (height, width): it is the sigma parameter of a Gaussian filter run on all the images during initialization. If the filter matches the neurons, you will get a much better estimate. `gSig` goes with `gSiz`, which is the kernel size (height and width in pixels) used for the filter. See the `GaussianBlur()` OpenCV function for more details on the sigma and size parameters.\n",
    "    \n",
    "   \n",
    "`merge_thr (float)`: *merge threshold* \n",
    "\n",
    "> If two spatially overlapping components are correlated above `merge_thr`, they will be merged into one component. The correlation coefficient is calculated using their calcium traces. If Caiman identifies a \"component\" that clearly contains two overlapping components, then increase `merge_thr`. \n",
    "\n",
    "You typically will set `rf` and `stride` infrequently, so `K`, `gSig`, and `merge_thr` are the main parameters you will tweak when analyzing a given session. Note these are not the *only* important parameters. They just tend to be the *most* important: the others tend to depend on your calcium indicator or other factors that don't vary within an experimental session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set dimensions of the video you want to analyze in pixels here\n",
    "dimensions = (490,490) #[default from video used in testing: (490,490)]\n",
    "\n",
    "# general dataset-dependent parameters\n",
    "fr = 1.28                 # imaging rate in frames per second [default: 1.28 for the file used in initial testing]\n",
    "decay_time = 20             # length of a typical transient in seconds [default: 20]\n",
    "dxy = (0.2635765, 0.2635765)# spatial resolution in x and y in (um per pixel) [default: (0.2635765, 0.2635765)]\n",
    "\n",
    "# motion correction parameters [I haven't tested different values for these]\n",
    "strides = (48, 48)          # start a new patch for pw-rigid motion correction every x pixels [default: (48, 48)]\n",
    "overlaps = (24, 24)         # overlap between patches (width of patch = strides+overlaps) [default: (24, 24)]\n",
    "max_shifts = (6,6)          # maximum allowed rigid shifts (in pixels) [default: (6,6)]\n",
    "max_deviation_rigid = 3     # maximum shifts deviation allowed for patch with respect to rigid shifts [default: 3]\n",
    "pw_rigid = True             # flag for performing non-rigid motion correction [default: True]\n",
    "\n",
    "# CNMF parameters for source extraction and deconvolution\n",
    "p = 1                       # order of the autoregressive system [default: 1] {from caiman documentation: (set p=2 if there is visible rise time in data)}\n",
    "gnb = 2                     # number of global background components (set to 1 or 2) [default: 2]\n",
    "merge_thr = 0.85            # merging threshold, max correlation allowed [default: 0.85]\n",
    "bas_nonneg = True          # enforce nonnegativity constraint on calcium traces (technically on baseline) [default: True]\n",
    "rf = 70                     # half-size of the patches in pixels (patch width is rf*2 + 1) [default: 70]\n",
    "stride_cnmf = rf            # amount of overlap between the patches in pixels (overlap is stride_cnmf+1) [default: stride_cnmf = rf]\n",
    "K = 20                      # number of maximum expected components per patch [default: 20]\n",
    "gSig = np.array([10, 10])   # expected half-width of neurons in pixels (Gaussian kernel standard deviation) [default: np.array([10, 10])]\n",
    "gSiz = 2*gSig + 1           # Gaussian kernel width and hight [default: 2*gSig + 1]\n",
    "method_init = 'greedy_roi'  # initialization method (if analyzing dendritic data see demo_dendritic.ipynb) [default: 'greedy_roi']\n",
    "ssub = 1                    # spatial subsampling during initialization [default: 1]\n",
    "tsub = 1                    # temporal subsampling during intialization [default: 1]\n",
    "\n",
    "# parameters for component evaluation\n",
    "min_SNR = 2.0               # signal to noise ratio for accepting a component [default: 2.0]\n",
    "rval_thr = 0.85             # space correlation threshold for accepting a component [default: 0.85]\n",
    "cnn_thr = 0.99              # threshold for CNN based classifier [default: 0.99]\n",
    "cnn_lowest = 0.05            # neurons with cnn probability lower than this value are rejected [default: 0.1]\n",
    "\n",
    "# uncategorized parameters\n",
    "rolling_sum = True #[default: True]\n",
    "only_init = True #[default: True]\n",
    "use_cnn = False #keep this always set to False. cnn does not work with our data because it was trained on spherical neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_dict = {'fnames': movie_path,\n",
    "                  'fr': fr,\n",
    "                  'dxy': dxy,\n",
    "                  'decay_time': decay_time,\n",
    "                  'strides': strides,\n",
    "                  'overlaps': overlaps,\n",
    "                  'max_shifts': max_shifts,\n",
    "                  'max_deviation_rigid': max_deviation_rigid,\n",
    "                  'pw_rigid': pw_rigid,\n",
    "                  'p': p,\n",
    "                  'nb': gnb,\n",
    "                  'rf': rf,\n",
    "                  'K': K, \n",
    "                  'gSig': gSig,\n",
    "                  'gSiz': gSiz,\n",
    "                  'stride': stride_cnmf,\n",
    "                  'method_init': method_init,\n",
    "                  'rolling_sum': rolling_sum,\n",
    "                  'only_init': only_init,\n",
    "                  'ssub': ssub,\n",
    "                  'tsub': tsub,\n",
    "                  'merge_thr': merge_thr, \n",
    "                  'bas_nonneg': bas_nonneg,\n",
    "                  'min_SNR': min_SNR,\n",
    "                  'rval_thr': rval_thr,\n",
    "                  'use_cnn': use_cnn,\n",
    "                  'min_cnn_thr': cnn_thr,\n",
    "                  'cnn_lowest': cnn_lowest}\n",
    "\n",
    "parameters = params.CNMFParams(params_dict=parameter_dict) # CNMFParams is the parameters class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" markdown=\"1\">\n",
    "    <h2 style=\"margin-top: 0;\">To dig deeper into this design</h2>  \n",
    "    To see more about the design of Caiman estimators and parameters, and their decoupling, see <a href=\"https://caiman.readthedocs.io/en/latest/Getting_Started.html#estimator-design\">our docs on estimator design</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up multicore processing\n",
    "Caiman is optimized for parallel computing, and distributes computations to multiple CPU cores for motion correction and CNMF. Setting up the multicore processing is done with the `setup_cluster()` function below. \n",
    "\n",
    "First, let's see how many CPUs we have available, and set the number of processors we want to use. If you set `num_processors_to_use` to `None`, then `setup_cluster()` will set it to the default of *one less* than the total number available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"You have {psutil.cpu_count()} CPUs available in your current environment\")\n",
    "num_processors_to_use = None #default: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a cluster of processors. If one has already been set up (the `cluster` variable is already in your namespace), then that cluster will be closed and a new one created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'cluster' in locals():  # 'locals' contains list of current local variables\n",
    "    print('Closing previous cluster')\n",
    "    cm.stop_server(dview=cluster)\n",
    "print(\"Setting up new cluster\")\n",
    "_, cluster, n_processes = cm.cluster.setup_cluster(backend='multiprocessing', \n",
    "                                                   n_processes=num_processors_to_use, \n",
    "                                                   ignore_preexisting=False)\n",
    "print(f\"Successfully initilialized multicore processing with a pool of {n_processes} CPU cores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `cluster` variable is the pool of processors (CPUs) that will be used in many of Caiman's subsequent processing steps. In these later steps, if you set the parameter `dview` to `cluster`, then parallel processing will be used. If instead you set `dview` to `None` then no parallel processing will be used. This latter option can be helpful when debugging, as the logger doesn't typically work for multi-CPU operations.\n",
    "\n",
    "For more details, please see [our documentation on cluster setup](https://caiman.readthedocs.io/en/latest/Getting_Started.html#cluster-setup-and-shutdown). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" markdown=\"1\">\n",
    "    <h2 style=\"margin-top: 0;\">Optimizing performance</h2>  \n",
    "    If you hit memory issues later, there are a few things you can do. First, you may want to lower the number of processors you are using. Each processor uses more RAM, and on a workstation with many processors, you can sometimes get better performance by reducing <em>num_processors_to_use</em>.The best way to determine the optimal number is by trial and error. When you set <em>num_processors_to_use</em> variable to <em>None</em>, it defaults to <i>one</i> less than the total number of CPU cores available (the reason we don't automatically set it to the total number of cores is because in practice this typically leads to worse performance).\n",
    "\n",
    "<br>Second, if your system has less than 32GB of RAM, and things are running slowly or you are running out of memory, then get more RAM. While you can sometimes get away with less, we recommend a *bare minimum* level of 16GB of RAM, but more is better. 32GB RAM is acceptable, 64GB or more is best. Obviously, this will depend on the size of your data sets.\n",
    "\n",
    "<br>Third, try subsampling your data. You can *spatially* or *temporally* subsample. If you are already sampling at a low frame rate, you should probably just spatially subsample. You can do this outside of Caiman (many acquisition systems have this capability built in), and load the subsampled file into the notebook directly. This will make subsequent analysis less prone to subtle errors. You can also set Caiman's `ssub` and `tsub` parameters (spatial and temporal subsampling parameters). If you do set `ssub` to 2, then you should divide `gSig` by 2. Dealing with such parameter side-effects is why it is easier to subsample *before* importing data into Caiman. If the results of your analysis with subsampled data look reasonable, then you are good to go. What if you can't subsample your data? \n",
    "\n",
    "<br>If none of the above memory optimization procedures work, you may just have too much data for offline CNMF. For this case, we also provide an online version of CNMF (OnACID), which uses a small number of frames to initialize the spatial and temporal components, and iteratively updates the components as new data comes in. This uses much less memory than the offline approach. The demo notebook for OnACID is found in <a href=\"./demo_OnACID_mesoscope.ipynb\">demo_OnACID_mesoscope.ipynb</a>. See the <a href=\"https://pubmed.ncbi.nlm.nih.gov/30652683/\">Caiman paper</a> for more discussion.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motion Correction\n",
    "The first substantive step in our analysis pipeline is to remove motion artifacts from the original movie:\n",
    "\n",
    "<img src=\"images/normcorre_workflow.jpg\" alt=\"motion correction workflow\" width=\"700\"/>\n",
    "\n",
    "It is *very* important to get rid of motion artifacts, as the subsequent CNMF source separation algorithm assumes that each pixel represents the same region of space\n",
    "\n",
    "First, we initialize the motion correction estimator using the parameters that we set above:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mot_correct = MotionCorrect(movie_path, dview=cluster, **parameters.motion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#%% Run piecewise-rigid motion correction using NoRMCorre\n",
    "mot_correct.motion_correct(save_movie=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the results: compare the original movie with the motion corrected movie. We are turning the gain up here to highlight motion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compare with original movie  : press q to quit\n",
    "movie_orig = cm.load(movie_path) # in case it was not loaded earlier\n",
    "movie_corrected = cm.load(mot_correct.mmap_file) # load motion corrected movie\n",
    "ds_ratio = 0.2\n",
    "cm.concatenate([movie_orig.resize(1, 1, ds_ratio) - mot_correct.min_mov*mot_correct.nonneg_movie,\n",
    "                movie_corrected.resize(1, 1, ds_ratio)], \n",
    "                axis=2).play(fr=30, \n",
    "                             gain=2, \n",
    "                             magnification=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_projection = np.max(movie_corrected, axis=0)\n",
    "correlation_image = cm.local_correlations(movie_corrected, swap_dim=False)\n",
    "correlation_image[np.isnan(correlation_image)] = 0 # get rid of NaNs, if they exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ((ax_max_orig, ax_max), (ax_corr_orig, ax_corr)) = plt.subplots(2,2,figsize=(6,6), sharex=True, sharey=True)\n",
    "# plot max projection\n",
    "ax_max_orig.imshow(max_projection_orig, \n",
    "                   cmap='viridis', \n",
    "                   vmin=np.percentile(np.ravel(max_projection_orig),50), \n",
    "                   vmax=np.percentile(np.ravel(max_projection_orig),99.5));\n",
    "ax_max_orig.set_title(\"Max Projection: Orig\", fontsize=12);\n",
    "ax_max.imshow(max_projection, \n",
    "              cmap='viridis', \n",
    "              vmin=np.percentile(np.ravel(max_projection),50), \n",
    "              vmax=np.percentile(np.ravel(max_projection),99.5));\n",
    "ax_max.set_title(\"Max Projection: Corrected\", fontsize=12);\n",
    "\n",
    "# plot correlation image\n",
    "ax_corr_orig.imshow(correlation_image_orig, \n",
    "                    cmap='viridis', \n",
    "                   vmin=np.percentile(np.ravel(correlation_image_orig),50), \n",
    "                   vmax=np.percentile(np.ravel(correlation_image_orig),99.5));\n",
    "ax_corr_orig.set_title('Correlation Im: Orig', fontsize=12);\n",
    "ax_corr.imshow(correlation_image, \n",
    "               cmap='viridis', \n",
    "               vmin=np.percentile(np.ravel(correlation_image),50), \n",
    "               vmax=np.percentile(np.ravel(correlation_image),99.5));\n",
    "ax_corr.set_title('Correlation Im: Corrected', fontsize=12);\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and accessing memory mapped files \n",
    "The next step is to handle the motion corrected file in memory using a *memory mapped* file. This lets us treat the data *as if* it were in memory while leaving it on disk (for more details about memory mapping, see [our documentation](https://caiman.readthedocs.io/en/latest/Getting_Started.html#memory-mapping))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "border_to_0 = 0 if mot_correct.border_nan == 'copy' else mot_correct.border_to_0 # trim border against NaNs\n",
    "mc_memmapped_fname = cm.save_memmap(mot_correct.mmap_file, \n",
    "                                        base_name='memmap_', \n",
    "                                        order='C',\n",
    "                                        border_to_0=border_to_0,  # exclude borders, if that was done\n",
    "                                        dview=cluster)\n",
    "\n",
    "Yr, dims, num_frames = cm.load_memmap(mc_memmapped_fname)\n",
    "images = np.reshape(Yr.T, [num_frames] + list(dims), order='F') #reshape frames in standard 3d format (T x X x Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restart the cluster to clean up memory in preparation for CNMF run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.stop_server(dview=cluster)\n",
    "_, cluster, n_processes = cm.cluster.setup_cluster(backend='multiprocessing', \n",
    "                                                   n_processes=num_processors_to_use, \n",
    "                                                   single_thread=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run CNMF on patches in parallel\n",
    "\n",
    "Everything is now set up for running CNMF. This algorithm simultaneously extracts the *spatial footprint* and corresponding *calcium trace* for each component. \n",
    "\n",
    "![cnmf patch flow image](images/cnmf_workflow.jpg)\n",
    "\n",
    "It also performs *deconvolution*, providing an estimate of the spike count that generated the calcium signal in the movie. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm is parallelized as illustrated here:\n",
    "\n",
    "<img src=\"images/cnmf_patches.jpg\" alt=\"cnmf patch flow\" width=\"500\"/>\n",
    "\n",
    "1) The movie field of view is split into overlapping patches.\n",
    "2) These patches are processed in parallel by the CNMF algorithm. The degree of parallelization depends on your available computing power: if you have just one CPU then the patches will be processed sequentially. \n",
    "3) The results from all the patches are merged, with special focus on components in overlapping regions -- overlapping components are merged if their activity is highly correlated.\n",
    "4) Results are refined with additional iterations of CNMF (the `refit()` algorithm is run). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed above, Caiman's main algorithms are run in two steps: first the estimators are *initialized* with a set of parameters, and then they are *fit* against actual data. Let's initialize our CNMF estimator object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnmf_model = cnmf.CNMF(n_processes, \n",
    "                       params=parameters, \n",
    "                       dview=cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting spatial parameters\n",
    "To select the spatial parameters (`gSig`, `rf`, `stride`, `K`), you need to look at your movie, or a summary image for your movie, and pick values close to those suggested by the guidelines above. It is helpful to use `view_quilt()` function to see if our key spatial parameters are in the right ballpark (note we recommend running this viewer in interactive qt mode so you can interact with it and get a better feel for the parameters):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate stride and overlap from parameters\n",
    "cnmf_patch_width = cnmf_model.params.patch['rf']*2 + 1\n",
    "cnmf_patch_overlap = cnmf_model.params.patch['stride'] + 1\n",
    "cnmf_patch_stride = cnmf_patch_width - cnmf_patch_overlap\n",
    "print(f'Patch width: {cnmf_patch_width} , Stride: {cnmf_patch_stride}, Overlap: {cnmf_patch_overlap}');\n",
    "\n",
    "# plot the patches\n",
    "patch_ax = view_quilt(correlation_image, \n",
    "                      cnmf_patch_stride, \n",
    "                      cnmf_patch_overlap, \n",
    "                      vmin=np.percentile(np.ravel(correlation_image),50), \n",
    "                      vmax=np.percentile(np.ravel(correlation_image),99.5),\n",
    "                      figsize=(4,4));\n",
    "patch_ax.set_title(f'CNMF Patches Width {cnmf_patch_width}, Overlap {cnmf_patch_overlap}');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run CNMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cnmf_fit = cnmf_model.fit(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly inspect the results by plotting contours of identified components using `plot_contours_nb()`. \n",
    "\n",
    "You can interactively explore this plot in your notebook with the help of the buttons on the right-hand-side of the plot (it was made using the [Bokeh](https://bokeh.org/) library). They let you zoom, pan, reset, or save the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnmf_fit.estimates.plot_contours_nb(img=correlation_image);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note this is just an initial result, which will contain many false positives, which is to be expected. The main concern to watch for here is whether you have lots of false *negatives* (has the algorithm missed neurons?). False negatives are hard to fix later, so if you have an unacceptable number, be sure to go back and re-run CNMF with new parameters.\n",
    "\n",
    "> If you get a data rate error with any notebook plotting commmands, can start your notebook using     \n",
    "`jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cnmf_refit = cnmf_fit.refit(images, dview=cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spatial contours of the new estimates should now look cleaner and more canonically neuronal in shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnmf_refit.estimates.plot_contours_nb(img=correlation_image);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The estimates class\n",
    "The main point of the CNMF algorithm is to perform source separation to extract the location of neurons and calcium activity traces from your raw data. This information, and many useful methods for visualization and analysis, are contained in Caiman's `Estimates` class. In the above code, an `estimates` object was generated when you ran the CNMF algorithm (you can find it in `cnmf_refit.estimates`). \n",
    "\n",
    "The rest of this notebook, from component evaluation to calculation of DFoF, is effectively an exploration of the properties and methods of the `Estimates` class. The most important estimates generated, which are properties of the `cnmf_refit.estimates`, are given in the following table:\n",
    "\n",
    "| Variable | Meaning | Shape |\n",
    "|:-------- |:------------- |:--------------------- |\n",
    "| **C** | Denoised calcium traces |  num components x num_frames |\n",
    "| **F_dff** | $\\Delta F/F$ |  num_components x num_frames |\n",
    "| **S** | Spike count estimate for each component from deconvolution, if used |  num_components x num_frames |\n",
    "| **YrA** | Residual for each trace |  num_components x num_frames |\n",
    "| **A** | Spatial components/footprints |  num pixels x num components |\n",
    "\n",
    "To recover raw calcium traces, you can add together the denoised calcium traces and their residuals (`C + YrA`). Note that the `F_dff` calculation is not done automatically, so when you run `fit()` the `F_dff` field will initially be `None`. Below, we will show how to populate that field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see shape of A and C\n",
    "cnmf_refit.estimates.A.shape, cnmf_refit.estimates.C.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnmf_refit.estimates.C.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" markdown=\"1\">\n",
    "    <h2 style=\"margin-top: 0;\">More on Estimates</h2>\n",
    "    The estimates object contains a great deal of information. The attributes are discussed in more detail <a href=\"https://caiman.readthedocs.io/en/latest/Getting_Started.html#result-interpretation\">in the documentation</a>, but you might also find exploring the <a href=\"https://github.com/flatironinstitute/CaImAn/blob/main/caiman/source_extraction/cnmf/estimates.py\">source code</a> helpful. For instance, while most users initially care about the extracted calcium signals <em>C</em> and spatial footprints <em>A</em>, the <b>background model</b> is also very important. The background model is included in the estimate in fields <em>b</em> and <em>f</em> (which correspond to the spatial and temporal components of the low-rank background model, respectively). We discuss the background model more below.\n",
    "\n",
    "<br>We realize that attribute names like <em>A</em> are not very informative or Pythonic. These names are rooted in mathematical conventions from the original papers in the literature.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Component Evaluation\n",
    "As already mentioned, the initial estimates produced by CNMF contains many spurious components. Our next step is to do some some quality control, cutting out the bad estimates to arrive at our final set of estimates:\n",
    "\n",
    "![component evaluation image](images/evaluation_workflow.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will evaluate each component by applying the `evaluate_components()` method. The criteria Caiman uses to evaluate components are:  \n",
    "\n",
    "- **Signal to noise ratio (SNR)**: a baseline noise estimate is extraced for each raw calcium trace, and SNR during calcium transients is calculated relative to this baseline. These values are stored in `estimates.SNR_comp`. Those components with high SNR are higher quality, and less likely to be false positives.\n",
    "- **Spatial correlation**: the extracted spatial footprints in `estimates.A` should be highly correlated with activity in the actual movie, at least on those frames when that component is active. These correlation coefficients are stored in `estimates.r_values`. \n",
    "- **CNN confidence**: Each spatial component in `estimates.A` is passed through a CNN-based classifier, trained on consensus data sets, that produces a confidence value between 0 and 1 that the shape is a real neuron. These are stored in `estimates.cnn_preds`.\n",
    "\n",
    "The first two criteria are illustrated schematically here (see also Figure 2 of <a href=\"https://elifesciences.org/articles/38173\">the Caiman paper</a>):\n",
    "\n",
    "![component evaluation image](images/component_evaluation.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `evaluate_components()` method uses the above criteria to sort components into accepted and rejected components. For each criterion, there is a threshold value in `quality` field of the parameters object -- the thresholds are `min_SNR`, `rval_thr`, and `min_cnn_thr`, respectively. If a unit is below *all* of those threshold values, it will be rejected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Thresholds to be used for evaluate_components()\")\n",
    "print(f\"min_SNR = {cnmf_refit.params.quality['min_SNR']}\")\n",
    "print(f\"rval_thr = {cnmf_refit.params.quality['rval_thr']}\")\n",
    "print(f\"min_cnn_thr = {cnmf_refit.params.quality['min_cnn_thr']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `estimates.evaluate_components()`. This can take a few minutes if you have a very large number of components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnmf_refit.estimates.evaluate_components(images, cnmf_refit.params, dview=cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method filled in two arrays in the `estimates` class: `idx_components` (accepted) and `idx_components_bad` (rejected). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Num accepted/rejected: {len(cnmf_refit.estimates.idx_components)}, {len(cnmf_refit.estimates.idx_components_bad)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div class=\"alert alert-info\" markdown=\"1\">\n",
    "    <h2 style=\"margin-top: 0;\">More on component evaluation</h2>  \n",
    "In practice, SNR is the most important evaluation factor. The spatial correlation factors are less important. In particular, the CNN for spatial evaluation may be inaccurate if your neural components are not \"canonically\" shaped somata. \n",
    "\n",
    "\n",
    "<br>When running <em>evaluate_components()</em> the three evaluation thresholds are appied <em>inclusively</em>: if a component is above <em>any</em> of the thresholds, it will pass muster. This was found in practice to be reasonable (e.g., a low SNR component that is very strongly neuronally shaped tends to not be an accident: it is just a very low SNR neuron). However, there is a second set of <b>absolute</b> threshold parameters  set for each criterion. If a component is <em>below</em> this absolute threshold for any of the evaluation parameters, it will be discarded: these are the <em>SNR_lowest</em>, <em>rval_lowest</em>, and <em>cnn_lowest</em>, respectively. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnmf_refit.estimates.plot_contours_nb(img=correlation_image, \n",
    "                                      idx=cnmf_refit.estimates.idx_components);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View individual spatial/temporal components\n",
    "One of the most useful visualization tools is `nb_view_components()`, which lets you scroll through individiual spatial and temporal components. This tool also displays the values of the three evaluation criteria for each component, which can be useful if you feel you need to change your evaluation criteria and re-run `evaluate_components()`. Perhaps you have too many false negatives and want to lower your SNR threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# view accepted components\n",
    "cnmf_refit.estimates.nb_view_components(img=correlation_image, \n",
    "                                        idx=cnmf_refit.estimates.idx_components,\n",
    "                                        cmap='gray',\n",
    "                                        denoised_color='red');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above shows the raw traces (`C+YrA`) by default, but you can superimpose the denoised traces from `C` if you add a color to the `denoised_color` parameter. As always in Jupyter, if you are unsure how a method works, you can enter `cnmf_refit.estimates.nb_view_components?` in a new cell to get the documentation for the method. \n",
    "\n",
    "We can also view the rejected compoonents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rejected components\n",
    "if len(cnmf_refit.estimates.idx_components_bad) > 0:\n",
    "    cnmf_refit.estimates.nb_view_components(img=correlation_image, \n",
    "                                            idx=cnmf_refit.estimates.idx_components_bad, \n",
    "                                            cmap='gray',\n",
    "                                            denoised_color='red')\n",
    "else:\n",
    "    print(\"No components were rejected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> One legacy from Matlab with these plotters is that they use one-based indexing when showing `Neuron number`. This can make it confusing when comparing to your own plotting results which will use zero-based indexing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building your own visualizations\n",
    "> This is a slightly more advanced section that you can safely skip your first time through. \n",
    "\n",
    "There are many custom visualizations you can build yourself based on the estimates generated from Caiman. For example, what if you wanted to plot the spatial footprint of a neuron with the contour superimposed?  The contours of the spatial footprints are in `estimates.coordinates`, which is a list of dictionaries corresponding to each component. The dictionary includes a `coordinates` field that contains the x,y coordinates of the contour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do not delete this portion of code. I don't know if I still need it, but I'm not touching it\n",
    "idx_accepted = cnmf_refit.estimates.idx_components\n",
    "all_contour_coords = [cnmf_refit.estimates.coordinates[idx]['coordinates'] for idx in idx_accepted]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each footprint in `A` is stored as a compressed sparse column array. We have extracted the calcium traces `C`, spatial footprints `A`, and estimated spike counts `S`, which is the main goal with CNMF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract $\\Delta F/F$ values\n",
    "So far our calcium traces are in arbitrary units. It is more common to report the calcium fluorescence relative to some baseline value $F_0$:\n",
    "\n",
    "$$\n",
    "\\Delta F/F = (F(t)-F_0)/F_0\n",
    "$$\n",
    "\n",
    "Traditionally, the baseline value was calculated during some initial period of queiscience (e.g., in the visual system, when the animal was sitting in the dark). However, it is problematic to make any assumptions about a \"quiet\" baseline period, so researchers have started to use a moving average to calculate $F_0$. This is also what Caiman does.\n",
    "\n",
    "More specifically, in Caiman the baseline is a running percentile calculated over a `frames_window` moving window. You can calculate $\\Delta F/F$ using raw traces or the denoised traces in C (this is toggled using the `use_residuals` argument)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cnmf_refit.estimates.F_dff is None:\n",
    "    print('Calculating estimates.F_dff')\n",
    "    cnmf_refit.estimates.detrend_df_f(quantileMin=8, \n",
    "                                      frames_window=250,\n",
    "                                      flag_auto=False,\n",
    "                                      use_residuals=False);  \n",
    "else:\n",
    "    print(\"estimates.F_dff already defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `estimates` object will now have a `F_dff` field, which makes it easier to compare traces across neurons/sessions because the data is normalized. Note that F_dff values can be positive or negative because it is relative to a baseline value that is the \"zero\" value.\n",
    "\n",
    "We can plot a temporal trace of a dff with time. In the following, we'll generate a `frame_time` variable for plotting. This is an  idealized time representation using `linspace()`, which assumes that images were acquired at a constant frequency throughout the session. In practice your hardware probably returns the time at which each frame was acquired, so you can use that information in your own analysis if you have it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_rate = cnmf_refit.params.data['fr']\n",
    "frame_pd = 1/frame_rate\n",
    "frame_times = np.linspace(0, num_frames*frame_pd, num_frames);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot F_dff\n",
    "idx_to_plot = 29\n",
    "idx_accepted = cnmf_refit.estimates.idx_components\n",
    "component_number = idx_accepted[idx_to_plot]\n",
    "f, ax = plt.subplots(figsize=(7,2))\n",
    "ax.plot(frame_times, \n",
    "        cnmf_refit.estimates.F_dff[component_number, :], \n",
    "        linewidth=0.5,\n",
    "        color='k');\n",
    "ax.set_xlabel('Time (s)')\n",
    "ax.set_ylabel('$\\Delta F/F$')\n",
    "ax.set_title(f\"$\\Delta F/F$ for unit {component_number}\");\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnmf_refit.estimates.nb_view_components(cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" markdown=\"1\">\n",
    "    <h2 style=\"margin-top: 0;\">More on $\\Delta F/F$</h2>  \n",
    "The above discussion of dff leaves out some details about it is actually calculated in Caiman. In practice, Caiman locally projects the model of the background activity to the spatial footprint of the neuron, and adds a moving percentile of this projected trace to the denominator as an additional normalizing term (see the discussion on the background model below). This acts as an empirical fudge factor that can keep $\\Delta F/F$ from getting too large for very small values of $F_0$ (especially when using denoised traces). Users may prefer to use the more traditional equation directly. Thanks to Peter Rupprecht for a helpful discussion of this topic. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select only accepted components\n",
    "We want to discard rejected components (`estimates.idx_components_bad`) from the `estimates` field, so we run  `select_components()`. This is useful because we only want to focus on the accepted components for downstream analysis .\n",
    "\n",
    "<div class=\"alert alert-warning\" markdown=\"1\">\n",
    "    <h4 style=\"margin-top: 0;\">Note: select_components() is a destructive operation</h4>  \n",
    "\n",
    "Running this command removes rejected components from the <em>estimates</em> field. They're not deleted completely, just moved to a different dictionary within the HDF5 file. If you think you might want them later, you can set the <em>save_discarded_components</em> parameter to <em>True</em>. This will let you retrieve them later with the <em>restore_discarded_components()</em> method. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnmf_refit.estimates.select_components(use_object=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `use_object` parameter specifies that we want to select the accepted components in `estimates.idx_components` (and remove the components in `idx_components_bad`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display final results\n",
    "View the final refined set of remaining components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnmf_refit.estimates.nb_view_components(img=correlation_image,\n",
    "                                        idx=None,\n",
    "                                        thr=0.99,\n",
    "                                        denoised_color='red',\n",
    "                                        cmap='gray',\n",
    "                                        );\n",
    "#if the destructive process that preceeds this is commented out, the bad components won't be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnmf_refit.estimates.plot_contours_nb(img=correlation_image,\n",
    "                                     idx=cnmf_refit.estimates.idx_components,\n",
    "                                     cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save summary images\n",
    "\n",
    "The following cell plots out the identified ROIs in different colors and saves one of the two images\n",
    "There is no significance to the colors or the different images. I found it easier to tell if the ROIs\n",
    "were accurate when plotting them in this way. The alternate image helps figure out if two adjoining ROIs\n",
    "of similar colors are different ROIs or the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#if the image scale is messed up, you need to set the dimensions of your video. Origionally set to 490x490\n",
    "dimensions = dimensions\n",
    "\n",
    "#pathing for where the file gets saved\n",
    "pretty_save_path = os.path.join(output_filename, 'pretty_plot')\n",
    "\n",
    "\n",
    "#I just chose as many colors as I needed to make sure there weren't too many overlaps\n",
    "colors = ['xkcd:red','xkcd:purple','xkcd:green',\n",
    "          'xkcd:blue','xkcd:pink','xkcd:orange',\n",
    "          'xkcd:yellow','xkcd:cyan','xkcd:wine',\n",
    "          'xkcd:leaf','xkcd:green yellow','xkcd:orange red',\n",
    "         'xkcd:butter','xkcd:pea soup']\n",
    "fig=plt.figure(frameon=False,figsize=dimensions,dpi=1)\n",
    "x=0\n",
    "while x < len(idx_accepted):\n",
    "    idx_to_plot = x\n",
    "    component_number = idx_accepted[idx_to_plot]\n",
    "    component_contour = all_contour_coords[idx_to_plot]\n",
    "    component_footprint = np.reshape(cnmf_refit.estimates.A[:, idx_to_plot].toarray(), dims, order='F')\n",
    "    plt.plot(component_contour[:, 0], \n",
    "             component_contour[:, 1], \n",
    "             color=colors[x % len(colors)], \n",
    "             linewidth=1,\n",
    "            alpha = 0.50)\n",
    "    plt.fill(component_contour[:, 0], \n",
    "             component_contour[:, 1], \n",
    "             color=colors[x % len(colors)], \n",
    "             linewidth=0.5,\n",
    "            alpha = 0.25)\n",
    "    x= x+1\n",
    "else:\n",
    "    plt.axis('off')\n",
    "    plt.imshow(correlation_image,cmap='gray');\n",
    "#don't get rid of any of the stuff here, need it to stop plt.savefig from messing up the image resolution\n",
    "    plt.subplots_adjust(top = 1, bottom = 0, right = 1, left = 0, \n",
    "            hspace = 0, wspace = 0)\n",
    "    plt.margins(0,0)\n",
    "    plt.gca().xaxis.set_major_locator(plt.NullLocator())\n",
    "    plt.gca().yaxis.set_major_locator(plt.NullLocator())\n",
    "    plt.savefig(pretty_save_path,bbox_inches=None,pad_inches=None,dpi='figure') \n",
    "\n",
    "colors = ['xkcd:purple','xkcd:wine',\n",
    "          'xkcd:blue','xkcd:pink','xkcd:cyan',\n",
    "          'xkcd:yellow','xkcd:orange','xkcd:green',\n",
    "          'xkcd:leaf','xkcd:green yellow','xkcd:orange red',\n",
    "         'xkcd:butter','xkcd:red']\n",
    "fig=plt.figure(frameon=False,figsize=dimensions,dpi=1)\n",
    "x=0\n",
    "while x < len(idx_accepted):\n",
    "    idx_to_plot = x\n",
    "    component_number = idx_accepted[idx_to_plot]\n",
    "    component_contour = all_contour_coords[idx_to_plot]\n",
    "    component_footprint = np.reshape(cnmf_refit.estimates.A[:, idx_to_plot].toarray(), dims, order='F')\n",
    "    plt.plot(component_contour[:, 0], \n",
    "             component_contour[:, 1], \n",
    "             color=colors[x % len(colors)], \n",
    "             linewidth=1,\n",
    "            alpha = 0.50)\n",
    "    plt.fill(component_contour[:, 0], \n",
    "             component_contour[:, 1], \n",
    "             color=colors[x % len(colors)], \n",
    "             linewidth=0.5,\n",
    "            alpha = 0.25)\n",
    "    x= x+1\n",
    "else:\n",
    "    plt.axis('off')\n",
    "    plt.imshow(correlation_image,cmap='gray');\n",
    "#don't get rid of any of the stuff here, need it to stop plt.savefig from messing up the image resolution\n",
    "    plt.subplots_adjust(top = 1, bottom = 0, right = 1, left = 0, \n",
    "            hspace = 0, wspace = 0)\n",
    "    plt.margins(0,0)\n",
    "    plt.gca().xaxis.set_major_locator(plt.NullLocator())\n",
    "    plt.gca().yaxis.set_major_locator(plt.NullLocator())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results to csv\n",
    "When we showed how to save/load the cnmf model above, that was for working with the entire estimator object (for instance if you want to save your work and start again later). Often users just want to save certain results such as the calcium traces in `C` to csv and do downstream analysis later. In Python it is easiest to do this using the Pandas package.\n",
    "\n",
    "The following shows how to save your calcium traces to a file called `C_traces.csv`, to the `caiman_data` folder. You can adapt this code to save other data fields such as dff. We will also save the frame times as a convenient reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporting a text file containing the parameters used for data analysis\n",
    "import csv\n",
    "output_file = os.path.join(output_filename, 'settings_list.txt')\n",
    "csv_file = output_file\n",
    "csv_columns = ['fnames',\n",
    "                  'fr',\n",
    "                  'dxy',\n",
    "                  'decay_time',\n",
    "                  'strides',\n",
    "                  'overlaps',\n",
    "                  'max_shifts',\n",
    "                  'max_deviation_rigid',\n",
    "                  'pw_rigid',\n",
    "                  'p',\n",
    "                  'nb',\n",
    "                  'rf',\n",
    "                  'K', \n",
    "                  'gSig',\n",
    "                  'gSiz',\n",
    "                  'stride',\n",
    "                  'method_init',\n",
    "                  'rolling_sum',\n",
    "                  'only_init',\n",
    "                  'ssub',\n",
    "                  'tsub',\n",
    "                  'merge_thr',\n",
    "                  'bas_nonneg',\n",
    "                  'min_SNR',\n",
    "                  'rval_thr',\n",
    "                  'use_cnn',\n",
    "                  'min_cnn_thr',\n",
    "                  'cnn_lowest']\n",
    "params_dict = {'fnames': movie_path,\n",
    "                  'fr': fr,\n",
    "                  'dxy': dxy,\n",
    "                  'decay_time': decay_time,\n",
    "                  'strides': strides,\n",
    "                  'overlaps': overlaps,\n",
    "                  'max_shifts': max_shifts,\n",
    "                  'max_deviation_rigid': max_deviation_rigid,\n",
    "                  'pw_rigid': pw_rigid,\n",
    "                  'p': p,\n",
    "                  'nb': gnb,\n",
    "                  'rf': rf,\n",
    "                  'K': K, \n",
    "                  'gSig': gSig,\n",
    "                  'gSiz': gSiz,\n",
    "                  'stride': stride_cnmf,\n",
    "                  'method_init': method_init,\n",
    "                  'rolling_sum': rolling_sum,\n",
    "                  'only_init': only_init,\n",
    "                  'ssub': ssub,\n",
    "                  'tsub': tsub,\n",
    "                  'merge_thr': merge_thr, \n",
    "                  'bas_nonneg': bas_nonneg,\n",
    "                  'min_SNR': min_SNR,\n",
    "                  'rval_thr': rval_thr,\n",
    "                  'use_cnn': use_cnn,\n",
    "                  'min_cnn_thr': cnn_thr,\n",
    "                  'cnn_lowest': cnn_lowest}\n",
    "index=0\n",
    "# Open the file in write mode ('w') with the full path\n",
    "with open(output_file, 'w') as f:\n",
    "    f.write('Analysis settings used')  # Write header to the file\n",
    "    for item in csv_columns:\n",
    "        line = f\"{item}\\t{params_dict[csv_columns[index]]}\\n\"\n",
    "        f.write(line)  # Write each line to the file\n",
    "        index = index + 1\n",
    "\n",
    "\n",
    "print(f\"Settings used saved to '{output_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving calcium traces\n",
    "data_to_save = np.vstack((frame_times, cnmf_refit.estimates.R)).T  # Transpose so time series are in columns\n",
    "save_df = pd.DataFrame(data_to_save)\n",
    "save_df.rename(columns={0:'time'}, inplace=True)\n",
    "# check out the dataframe\n",
    "save_df.head()\n",
    "c_save_path = os.path.join(output_filename, 'C_traces.csv')\n",
    "save_df.to_csv(c_save_path, index=False)\n",
    "print(f\"Saved estimates.C to {c_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnmf_refit.estimates.nb_view_components()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving normalized transmittance\n",
    "data_to_save = np.vstack((frame_times, cnmf_refit.estimates.C)).T  # Transpose so time series are in columns\n",
    "save_df = pd.DataFrame(data_to_save)\n",
    "save_df.rename(columns={0:'time(s)'}, inplace=True)\n",
    "# Insert an empty column at the first position\n",
    "save_df.insert(0, '', np.nan)\n",
    "# check out the dataframe\n",
    "save_df.head()\n",
    "\n",
    "# Rename the remaining columns to Roi1, Roi2, etc.\n",
    "for i in range(1, len(save_df.columns) - 1):  # start from 1 to skip 'Empty Column'\n",
    "    save_df.rename(columns={i: f'Roi{i}'}, inplace=True)\n",
    "\n",
    "\n",
    "c_save_path = os.path.join(output_filename, 'ROI normalized.txt')\n",
    "save_df.to_csv(c_save_path, sep='\\t', index=False)\n",
    "print(f\"Saved estimates.ROI normalized to {c_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#F_dff WITHOUT headers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming frame_times and cnmf_refit.estimates.C are defined\n",
    "frame_times = np.array(frame_times)  # Convert to numpy array if not already\n",
    "C = np.array(cnmf_refit.estimates.C)  # Convert to numpy array if not already\n",
    "\n",
    "# Create DataFrame and ensure numeric values\n",
    "data_to_save = np.vstack((frame_times, C)).T  # Transpose to have time series in columns\n",
    "save_df = pd.DataFrame(data_to_save)\n",
    "\n",
    "# Insert an empty column at the first position\n",
    "save_df.insert(0, '', np.nan)\n",
    "\n",
    "# Convert all columns to numeric, setting errors='coerce' to handle non-numeric values\n",
    "save_df = save_df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Remove the empty column if not needed\n",
    "save_df.drop(columns=save_df.columns[0], inplace=True)\n",
    "\n",
    "Fdiff_save_path = os.path.join(output_filename, 'F_dff_simplified.txt')\n",
    "\n",
    "# Save DataFrame again without headers and index\n",
    "save_df.to_csv(Fdiff_save_path, sep='\\t', header=False, index=False)\n",
    "print(f\"Saved estimates.F_dff to {Fdiff_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving spike trains\n",
    "data_to_save = np.vstack((frame_times, cnmf_refit.estimates.S)).T  # Transpose so time series are in columns\n",
    "save_df = pd.DataFrame(data_to_save)\n",
    "save_df.rename(columns={0:'time'}, inplace=True)\n",
    "# check out the dataframe\n",
    "save_df.head()\n",
    "\n",
    "c_save_path = os.path.join(output_filename, 'S.csv')\n",
    "\n",
    "save_df.to_csv(c_save_path, index=False)\n",
    "print(f\"Saved estimates.S to {c_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Deconvolution of Spike Activity**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0:\n",
    "### *Imports*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sys import path\n",
    "path.append('..')\n",
    "from oasis.functions import gen_data, gen_sinusoidal_data, deconvolve, estimate_parameters\n",
    "from oasis.plotting import simpleaxis\n",
    "from oasis.oasis_methods import oasisAR1, oasisAR1, constrained_oasisAR1\n",
    "from caiman.source_extraction.cnmf.deconvolution import constrained_foopsi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2:\n",
    "### Create Plotting Function\n",
    "*written by the same person who wrote Oasis*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE PLOTTING FUNCTION\n",
    "def plot_trace(groundtruth=False, roi_idx=None):\n",
    "    plt.figure(figsize=(20,4))\n",
    "    plt.subplot(211)\n",
    "    plt.title(f'ROI {roi_idx + 1} - Deconvolved Fluorescence and Spikes')\n",
    "    plt.plot(b+c, lw=2, label='denoised')\n",
    "    if groundtruth:\n",
    "        plt.plot(true_b+true_c, c='r', label='truth', zorder=-11)\n",
    "    plt.plot(y, label='data', zorder=-12, c='y')\n",
    "    plt.legend(ncol=3, frameon=False, loc=(.02,.85))\n",
    "\n",
    "       # Set a fixed y-axis limit for fluorescence plot\n",
    "    #plt.ylim(0, 1000)  # Adjust this limit based on what you expect your data to look \n",
    "    \n",
    "    simpleaxis(plt.gca())\n",
    "    \n",
    "    plt.subplot(212)\n",
    "    plt.plot(s, lw=2, label='deconvolved', c='g')\n",
    "    if groundtruth:\n",
    "        for k in np.where(true_s)[0]:\n",
    "            plt.plot([k,k],[-.1,1], c='r', zorder=-11, clip_on=False)\n",
    "    plt.ylim(0,1.3)\n",
    "    plt.legend(ncol=3, frameon=False, loc=(.02,.85));\n",
    "    simpleaxis(plt.gca())\n",
    "    print(\"Correlation of deconvolved activity  with ground truth ('spikes') : %.4f\" % np.corrcoef(s,true_s)[0,1])\n",
    "    print(\"Correlation of denoised fluorescence with ground truth ('calcium'): %.4f\" % np.corrcoef(c,true_c)[0,1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3:\n",
    "### Plot  & Deconvolve Each ROI Individually *& Groundtruth*\n",
    "### **`constrained_foopsi`** Estimation of Spike Trains and Calicium Concentration\n",
    "Because of the way ROI's and their respective spiketrains are stored in a 2D array within `cnmf_refit.estimates.C`, we need to use a for loop to extract out the data.\n",
    "\n",
    "*The code below was generated by Chat GPT and edited by Devin Wilson*\n",
    "\n",
    "**Relevant Variables**\n",
    "- `y`  = the raw fluorescence data imported from `cnmf_refit.estimates.C`\n",
    "- `true_c` = Ground truth calcium signal\n",
    "- `true_s` = Ground truth spike train\n",
    "- `true_b` = Ground truth baseline\n",
    "\n",
    "\n",
    "**THE CODE BELOW ALSO EXPORTS THE SPIKES INTO A .CSV FILE CALLED `OUTPUTS.CSV`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path for outputs.csv\n",
    "csv_save_path = os.path.join(output_filename, 'roi_spikes.csv')\n",
    "\n",
    "# List to store spike data for each ROI\n",
    "spike_data = []\n",
    "\n",
    "for roi_idx in range(cnmf_refit.estimates.C.shape[0]):  # Loop over each ROI\n",
    "    # Define all the variables that we will be using\n",
    "    y = np.squeeze(cnmf_refit.estimates.C[roi_idx, :])  # Raw fluorescence data\n",
    "    b = None  # Baseline concentration (can be adjusted or set to None)\n",
    "    c1 = None  # Initial concentration (can be adjusted or set to None)\n",
    "    g = [.97]  # Discrete time constant (scalar or list for AR(2))\n",
    "    sn = .95  # Noise standard deviation (scalar)\n",
    "\n",
    "    # Call constrained_foopsi with specified arguments\n",
    "    try:\n",
    "        c, b, c1, g, sn, sp, lam = constrained_foopsi(y, b=b, g=g, sn=sn, p=1)\n",
    "        true_c = c\n",
    "        true_b = b\n",
    "\n",
    "        #plot_trace(True, roi_idx) #uncomment to show plots\n",
    "\n",
    "        # Perform deconvolution to get spike train\n",
    "        c, s = oasisAR1(y, g, lam, s_min=20)\n",
    "        true_s = s\n",
    "\n",
    "        # Append the entire spike train for this ROI to spike_data\n",
    "        spike_data.append(s)  # s is an array of spike values for this ROI over time\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing ROI {roi_idx + 1}: {e}\")\n",
    "    \n",
    "\n",
    "# Transpose spike_data so that each column corresponds to one ROI\n",
    "spike_data = np.array(spike_data).T\n",
    "\n",
    "# Create the ROI headers\n",
    "headers = [f'roi_{roi_idx + 1}' for roi_idx in range(cnmf_refit.estimates.C.shape[0])]\n",
    "\n",
    "# Write the spike data to CSV file with headers\n",
    "with open(csv_save_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    # Write the header row\n",
    "    writer.writerow(headers)\n",
    "    \n",
    "    # Write each row of spike data (each row corresponds to one time point, each column to one ROI)\n",
    "    writer.writerows(spike_data)\n",
    "\n",
    "print(f\"Successfully created roi_spikes.csv at {csv_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize the spike values for each ROI and output them to a .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path for output_summary.csv\n",
    "csv_summary_path = os.path.join(output_filename, 'roi_spike_sums.csv')\n",
    "\n",
    "# Assume `all_spike_data` is a list where each element is an array of spikes for each ROI\n",
    "all_spike_data = []\n",
    "\n",
    "for roi_idx in range(cnmf_refit.estimates.C.shape[0]):  # Loop over each ROI\n",
    "    y = np.squeeze(cnmf_refit.estimates.C[roi_idx, :])  # Raw fluorescence data\n",
    "    b = None  # Baseline concentration (can be adjusted or set to None)\n",
    "    c1 = None  # Initial concentration (can be adjusted or set to None)\n",
    "    g = [.95]  # Discrete time constant (scalar or list for AR(2))\n",
    "    sn = .95  # Noise standard deviation (scalar)\n",
    "\n",
    "    try:\n",
    "        # Perform constrained_foopsi deconvolution\n",
    "        c, b, c1, g, sn, sp, lam = constrained_foopsi(y, b=b, g=g, sn=sn, p=1)\n",
    "        all_spike_data.append(sp)  # Append spike data for this ROI\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing ROI {roi_idx + 1}: {e}\")\n",
    "\n",
    "# Compute the sum of values for each ROI\n",
    "summarized_data = [np.sum(spike_array) for spike_array in all_spike_data]\n",
    "\n",
    "# Write to CSV in the specified outputs folder\n",
    "with open(csv_summary_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    # Write a header\n",
    "    writer.writerow(['ROI', 'Summed Spike Values'])\n",
    "    \n",
    "    # Write each ROI's summed spike values in a new row\n",
    "    for roi_idx, spike_array in enumerate(all_spike_data):\n",
    "        summed_value = np.sum(spike_array)\n",
    "        writer.writerow([f'ROI_{roi_idx + 1}', summed_value])\n",
    "\n",
    "print(f\"Successfully created roi_spike_sums.csv at {csv_summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Spike Devonvolution (Deprecated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Set background transmittance threshold  [default used in testing: 0.001]\n",
    "#[omit any spike activity from cnmf_refit.estimates.S with normalized intensity at or below this threshold]\n",
    "#this was done because of a large number of near-zero values that are likely due to accumulated arithmatic error\n",
    "spike_thr = 0.001\n",
    "\n",
    "#make csv file with all spikes\n",
    "output_file = output_filename + 'S_spikes.txt'\n",
    "\n",
    "\n",
    "#clear and define needed variables\n",
    "test_roi = 0\n",
    "roi_num = 0\n",
    "s_start = 0\n",
    "s_dur = 0\n",
    "float_sum=0\n",
    "roi_label=0\n",
    "roi_x=0\n",
    "roi_y=0\n",
    "starttime=0\n",
    "hightime=0\n",
    "highamp=0\n",
    "\n",
    "\n",
    "#assembling an array for the ROI centers\n",
    "centers = np.array([['neuron_id','x','y']])\n",
    "for item in cnmf_refit.estimates.coordinates:\n",
    "    #insert stuff for time of peak response\n",
    "    CoM_coords = item['CoM']\n",
    "    line = np.array([[item['neuron_id'],CoM_coords[1],CoM_coords[0]]])\n",
    "    centers = np.concatenate((centers,line),axis=0)\n",
    "\n",
    "\n",
    "\n",
    "#start of spike extraction\n",
    "with open(output_file, 'w') as f:\n",
    "    f.write('ROI#\\t spike #\\t CoM x(px)\\t CoM y(px)\\t start of spike(frames from start of video)\\t time at max spike amplitude(ffsov)\\t duration(frames)\\t max amplitude\\t sum of amplitudes\\n')# Write header to the file\n",
    "    while roi_num < len(cnmf_refit.estimates.S):\n",
    "        test_roi = np.vstack((frame_times, cnmf_refit.estimates.S[roi_num])).T;\n",
    "        #select one ROI at a time and transpose so data is in format of [frame time, normalized intensity]\n",
    "        l=0\n",
    "        spike_idx=0\n",
    "        roi_label=roi_num+1 #set value for ROI that matches other caiman outputs (columns in c_traces for example)\n",
    "        roi_x=centers[roi_num+1][1] #set x value of the center for the ROI analyzed\n",
    "        roi_y=centers[roi_num+1][2] #set y value of the CoM for ROI being analyzed\n",
    "        while l < len(test_roi): #start loop => will scan through data extracted by Caiman from video frame by frame\n",
    "            if test_roi[l][1]>spike_thr: #if amplitude is above threshold, start spike math loops\n",
    "                spike_idx=spike_idx+1 #set identifier for detected spike\n",
    "                s_start=l #set start frame of detected spike\n",
    "                while test_roi[l][1]>spike_thr:\n",
    "                    float_sum=float_sum+test_roi[l][1] #rolling sum of all amplitudes during spike\n",
    "                    if test_roi[l][1]>highamp: #find point of highest activity \n",
    "                        highamp=test_roi[l][1] #set activity value for peak\n",
    "                        hightime=l #set time at peak in frames\n",
    "                    l=l+1 #advance 1 frame\n",
    "                    if l == len(test_roi): break #breaks this loop if at end of movie\n",
    "                else: \n",
    "                    #s_dur = test_roi[l][0]-test_roi[s_start][0] #for seconds instead of frames\n",
    "                    s_dur = l-s_start; #for frames instead of seconds\n",
    "                    #starttime=test_roi[s_start][0] #for seconds instead of frames\n",
    "                    starttime=s_start #for frames instead of seconds\n",
    "                    #hightime=test_roi[hightime][0] #convert time of peak amplitude to seconds\n",
    "                    line=f\"{roi_label}\\t{spike_idx}\\t{roi_x}\\t{roi_y}\\t{starttime}\\t{hightime}\\t{s_dur}\\t{highamp}\\t{float_sum}\\n\" #assemble the components extracted earlier in the loop\n",
    "                    f.write(line) #record components to file\n",
    "                    #print(line); #prints spike summary values for user to check in the next cell\n",
    "                    float_sum=0; #reset sum\n",
    "                    highamp=0; #reset highest amplitude within spike\n",
    "                    hightime=0; #reset time at highest amplitude within spike\n",
    "                    l = l+1; #advance one frame before starting to check for activity\n",
    "                    if l == len(test_roi): break #if at the end of the video, break this loop\n",
    "            else:\n",
    "                l=l+1\n",
    "                if l == len(test_roi): break\n",
    "        roi_num = roi_num+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Spiketrain summary data saved to '{output_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Spike extraction using Delta F/F0 values\n",
    "## Set background transmittance threshold [default 0. Will have errors if below 0]\n",
    "#[omit any spike activity from cnmf_refit.estimates.F_dff with normalized intensity at or below this threshold]\n",
    "#this was done because of a large number of near-zero values that are likely due to accumulated arithmatic error\n",
    "spike_thr = 0\n",
    "\n",
    "#make csv file with all spikes\n",
    "output_file = output_filename + 'dela_F_spikes.txt'\n",
    "\n",
    "\n",
    "#clear and define needed variables\n",
    "test_roi = 0\n",
    "roi_num = 0\n",
    "s_start = 0\n",
    "s_dur = 0\n",
    "float_sum=0\n",
    "roi_label=0\n",
    "roi_x=0\n",
    "roi_y=0\n",
    "starttime=0\n",
    "hightime=0\n",
    "highamp=0\n",
    "\n",
    "\n",
    "#assembling an array for the ROI centers\n",
    "centers = np.array([['neuron_id','x','y']])\n",
    "for item in cnmf_refit.estimates.coordinates:\n",
    "    #insert stuff for time of peak response\n",
    "    CoM_coords = item['CoM']\n",
    "    line = np.array([[item['neuron_id'],CoM_coords[1],CoM_coords[0]]])\n",
    "    centers = np.concatenate((centers,line),axis=0)\n",
    "\n",
    "#start of spike extraction\n",
    "with open(output_file, 'w') as f:\n",
    "    f.write('ROI#\\t spike #\\t CoM x(px)\\t CoM y(px)\\t start of spike(frames from start of video)\\t time at max spike amplitude(ffsov)\\t duration(frames)\\t max amplitude\\t sum of amplitudes\\n')# Write header to the file\n",
    "    print('ROI#\\t spike #\\t CoM x(px)\\t CoM y(px)\\t start of spike(frames from start of video)\\t time at max spike amplitude(ffsov)\\t duration(frames)\\t max amplitude\\t sum of amplitudes\\n')\n",
    "    while roi_num < len(cnmf_refit.estimates.F_dff):\n",
    "        test_roi = np.vstack((frame_times, cnmf_refit.estimates.F_dff[roi_num])).T;\n",
    "        #select one ROI at a time and transpose so data is in format of [frame time, normalized intensity]\n",
    "        l=0\n",
    "        spike_idx=0\n",
    "        roi_label=roi_num+1 #set value for ROI that matches other caiman outputs (columns in c_traces for example)\n",
    "        roi_x=centers[roi_num+1][1] #set x value of the center for the ROI analyzed\n",
    "        roi_y=centers[roi_num+1][2] #set y value of the CoM for ROI being analyzed\n",
    "        while l < len(test_roi): #start loop => will scan through data extracted by Caiman from video frame by frame\n",
    "            if test_roi[l][1]>spike_thr: #if amplitude is above threshold, start spike math loops\n",
    "                spike_idx=spike_idx+1 #set identifier for detected spike\n",
    "                s_start=l #set start frame of detected spike\n",
    "                while test_roi[l][1]>spike_thr:\n",
    "                    float_sum=float_sum+test_roi[l][1] #rolling sum of all amplitudes during spike\n",
    "                    if test_roi[l][1]>highamp: #find point of highest activity \n",
    "                        highamp=test_roi[l][1] #set activity value for peak\n",
    "                        hightime=l #set time at peak in frames\n",
    "                    l=l+1 #advance 1 frame\n",
    "                    if l == len(test_roi): break #breaks this loop if at end of movie\n",
    "                else: \n",
    "                    #s_dur = test_roi[l][0]-test_roi[s_start][0] #for seconds instead of frames\n",
    "                    s_dur = l-s_start; #for frames instead of seconds\n",
    "                    ##starttime=test_roi[s_start][0] #for seconds instead of frames\n",
    "                    starttime=s_start #for frames instead of seconds\n",
    "                    ##hightime=test_roi[hightime][0] #convert time of peak amplitude to seconds\n",
    "                    line=f\"{roi_label}\\t{spike_idx}\\t{roi_x}\\t{roi_y}\\t{starttime}\\t{hightime}\\t{s_dur}\\t{highamp}\\t{float_sum}\\n\" #assemble the components extracted earlier in the loop\n",
    "                    f.write(line) #record components to file\n",
    "                    #print(line); #prints spike summary values for user to check in the next cell\n",
    "                    float_sum=0; #reset sum\n",
    "                    highamp=0; #reset highest amplitude within spike\n",
    "                    hightime=0; #reset time at highest amplitude within spike\n",
    "                    l = l+1; #advance one frame before starting to check for activity\n",
    "                    if l == len(test_roi): break #if at the end of the video, break this loop\n",
    "            else:\n",
    "                l=l+1\n",
    "                if l == len(test_roi): break\n",
    "        roi_num = roi_num+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Delta F spikes summary data saved to '{output_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[125], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#assembling an array for the ROI centers\u001b[39;00m\n\u001b[1;32m     30\u001b[0m centers \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneuron_id\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcnmf_refit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimates\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoordinates\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#insert stuff for time of peak response\u001b[39;49;00m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mCoM_coords\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCoM\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mneuron_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mCoM_coords\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mCoM_coords\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "### Spike extraction using Delta F/F0 values\n",
    "## Set background transmittance threshold [default 0. Will have errors if below 0]\n",
    "#[omit any spike activity from cnmf_refit.estimates.F_dff with normalized intensity at or below this threshold]\n",
    "#this was done because of a large number of near-zero values that are likely due to accumulated arithmatic error\n",
    "spike_thr = 0\n",
    "\n",
    "#make csv file with all spikes\n",
    "output_file = output_filename + 'RAAIM_DF_UnverifiedSpread.txt'\n",
    "\n",
    "\n",
    "#clear and define needed variables\n",
    "test_roi = 0\n",
    "roi_num = 0\n",
    "s_start = 0\n",
    "s_dur = 0\n",
    "float_sum=0\n",
    "roi_label=0\n",
    "roi_x=0\n",
    "roi_y=0\n",
    "starttime=0\n",
    "hightime=0\n",
    "highamp=0\n",
    "S_AUC=0\n",
    "decaytime=0\n",
    "s_atk=0\n",
    "s_dky=0\n",
    "pxspread=0\n",
    "\n",
    "#assembling an array for the ROI centers\n",
    "centers = np.array([['neuron_id','x','y']])\n",
    "for item in cnmf_refit.estimates.coordinates:\n",
    "    #insert stuff for time of peak response\n",
    "    CoM_coords = item['CoM']\n",
    "    line = np.array([[item['neuron_id'],CoM_coords[1],CoM_coords[0]]])\n",
    "    centers = np.concatenate((centers,line),axis=0)\n",
    "\n",
    "#start of spike extraction\n",
    "with open(output_file, 'w') as f:\n",
    "    f.write('ROI\\tX\\tY\\tAmp(F/F0)\\tTime(s)\\tduration(s)\\tattack(s)\\tdecay(s)\\tA.U.C.(F*s/F0)\\tSpread(pixels^2)\\n')# Write header to the file\n",
    "    print('ROI\\tX\\tY\\tAmp(F/F0)\\tTime(s)\\tduration(s)\\tattack(s)\\tdecay(s)\\tA.U.C.(F*s/F0)\\tSpread(pixels^2)\\n')\n",
    "    while roi_num < len(cnmf_refit.estimates.F_dff):\n",
    "        test_roi = np.vstack((frame_times, cnmf_refit.estimates.F_dff[roi_num])).T;\n",
    "        #select one ROI at a time and transpose so data is in format of [frame time, normalized intensity]\n",
    "        l=0\n",
    "        spike_idx=0\n",
    "        roi_label=roi_num+1 #set value for ROI that matches other caiman outputs (columns in c_traces for example)\n",
    "        roi_x=centers[roi_num+1][1] #set x value of the center for the ROI analyzed\n",
    "        roi_y=centers[roi_num+1][2] #set y value of the CoM for ROI being analyzed\n",
    "        while l < len(test_roi): #start loop => will scan through data extracted by Caiman from video frame by frame\n",
    "            if test_roi[l][1]>spike_thr: #if amplitude is above threshold, start spike math loops\n",
    "                spike_idx=spike_idx+1 #set identifier for detected spike\n",
    "                s_start=l #set start frame of detected spike\n",
    "                while test_roi[l][1]>spike_thr:\n",
    "                    float_sum=float_sum+test_roi[l][1] #rolling sum of all amplitudes during spike\n",
    "                    if test_roi[l][1]>highamp: #find point of highest activity \n",
    "                        highamp=test_roi[l][1] #set activity value for peak\n",
    "                        hightime=l #set time at peak in frames\n",
    "                    l=l+1 #advance 1 frame\n",
    "                    if l == len(test_roi): break #breaks this loop if at end of movie\n",
    "                else: \n",
    "                    s_dur = test_roi[l][0]-test_roi[s_start][0] #for seconds instead of frames\n",
    "                    starttime=test_roi[s_start][0] #for seconds instead of frames\n",
    "                    s_atk=test_roi[hightime][0]-test_roi[s_start][0]\n",
    "                    hightime=test_roi[hightime][0] #convert time of peak amplitude to seconds\n",
    "                    S_AUC=s_dur*float_sum\n",
    "                    s_dky=s_dur-s_atk\n",
    "                    component_contour = all_contour_coords[roi_num]\n",
    "                    pxspread=len(component_contour)\n",
    "                    line=f\"{roi_label}\\t{roi_x}\\t{roi_y}\\t{highamp}\\t{hightime}\\t{s_dur}\\t{s_atk}\\t{s_dky}\\t{S_AUC}\\t{pxspread}\\n\" #assemble the components extracted earlier in the loop\n",
    "                    f.write(line) #record components to file\n",
    "                    print(line); #prints spike summary values for user to check in the next cell\n",
    "                    float_sum=0; #reset sum\n",
    "                    highamp=0; #reset highest amplitude within spike\n",
    "                    hightime=0; #reset time at highest amplitude within spike\n",
    "                    l = l+1; #advance one frame before starting to check for activity\n",
    "                    if l == len(test_roi): break #if at the end of the video, break this loop\n",
    "            else:\n",
    "                l=l+1\n",
    "                if l == len(test_roi): break\n",
    "        roi_num = roi_num+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the ROI list\n",
    "\n",
    "The following code was in-house and creates a list of important values for the identified calcium traces\n",
    "Components: ROI index number, center of mass coordinates, \n",
    "\n",
    "Written / adapted from existing code in the summer of 2024 by Grant Thomas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[122], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      8\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex(neuron ID)\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mX\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mY\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Write header to the file\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcnmf_refit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimates\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoordinates\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCoM_coords\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCoM\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mneuron_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mCoM_coords\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mCoM_coords\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "#import os\n",
    "#selecting directory and setting filename\n",
    "output_file = output_filename + 'accepted_ROIs_list.txt'\n",
    "\n",
    "CoM_coords = 0\n",
    "# Open the file in write mode ('w') with the full path\n",
    "with open(output_file, 'w') as f:\n",
    "    f.write('index(neuron ID)\\tX\\tY\\n')  # Write header to the file\n",
    "    for item in cnmf_refit.estimates.coordinates:\n",
    "        CoM_coords = item['CoM']\n",
    "        line = f\"{item['neuron_id']}\\t{CoM_coords[1]}\\t{CoM_coords[0]}\\n\"\n",
    "        f.write(line)  # Write each line to the file\n",
    "\n",
    "print(f\"Data saved to '{output_file}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View different result movies\n",
    "### An aside on the underlying model\n",
    "To understand the next two visualizations, we need to discuss the model used by Caiman a little bit. The CNMF algorithm models the original movie as a sum of *neural activity* and *background activity*: the *noise* (or *residual*) is everything left out:\n",
    "\n",
    "    original_movie = neural_activity + background + residual\n",
    "    \n",
    "In this model, `neural_activity` is the product of the matrices `AC`, the spatial and temporal components we have been exploring (`estimates.A` and `estimates.C`). It is our model of the neural bits that we care about.\n",
    "\n",
    "`background` is the model's representation of all the background activity in our movie that we wish wasn't there -- this includes stray fluorescence from the neuropil and out-of-plane neural components. This background model is also broken up into spatial and temporal components, which are in `estimates.b` (background) and `estimates.f` (fluctuations), respectively. \n",
    "\n",
    "The \"noise\", or residual term, is by definition, everything else not captured by the model: \n",
    "\n",
    "    residual = original_movie - neural_activity - background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View denoised movie\n",
    "We can rearrange the equations above to yield a \"denoised\" movie, which is just the original movie with the residual removed:\n",
    "\n",
    "    denoised_movie = original_movie - residual = neural_activity + background\n",
    "    \n",
    "Plugging in appropriate terms from the models of neural activity and background activity (`AC` and `bf`) yields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct denoised movie\n",
    "neural_activity = cnmf_refit.estimates.A @ cnmf_refit.estimates.C  # AC\n",
    "background = cnmf_refit.estimates.b @ cnmf_refit.estimates.f  # bf\n",
    "denoised_movie = neural_activity + background  # AC + bf\n",
    "\n",
    "# turn into a movie object\n",
    "denoised_movie = cm.movie(denoised_movie).reshape(dims + (-1,), order='F').transpose([2, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the denoised movie (with background included):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#press q to quit\n",
    "downsampling_ratio = 0.2\n",
    "denoised_movie.resize(fz=downsampling_ratio).play(gain=0.8,\n",
    "                                                  q_min=30,\n",
    "                                                  q_max=99, \n",
    "                                                  fr=30,\n",
    "                                                  plot_text=True,\n",
    "                                                  magnification=3,\n",
    "                                                  backend='opencv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize data, predicted activity, and residual\n",
    "For our final visualization, we will use a built-in method `play_movie()` that shows the original movie, the predicted movie (either `AC` or `AC + bf`), and the residual. \n",
    "\n",
    "Viewing the residuals (what the model doesn't explain) can be extremely useful: if you end up seeing lots of neural activity in the residual movie, that means your model is leaving something important out, and you might need to go tweak some parameters. In fact, in Caiman's online algorithms, this is how we decide whether to add new neurons after initialization! If neural activity is discovered in the residual buffer, then it's time to add a new neuron to `AC`! In the offline algorithms, it just means you might need to go check your parameters. No model is perfect: there is always some residual. You have to use your judgment about whether it is worth chasing with additional fitting.\n",
    "\n",
    "> The `play_movie()` method has an option to include the background from the model (`bf`) or not (this is the `include_bck` Boolean parameter). If you set it to `False`, it will subtract `bf` from the original movie and will only include `AC` in the middle panel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from caiman.base.movies import play_movie\n",
    "\n",
    "# in case you are working from loaded data, recover the raw movie\n",
    "Yr, dims, num_frames = cm.load_memmap(cnmf_refit.mmap_file)\n",
    "images = np.reshape(Yr.T, [num_frames] + list(dims), order='F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# press q to quit (can take a while to start running)\n",
    "cnmf_refit.estimates.play_movie(images, \n",
    "                                q_max=99.9, \n",
    "                                gain_res=1,\n",
    "                                magnification=1,\n",
    "                                include_bck=True,\n",
    "                                use_color=True,\n",
    "                                frame_range=slice(None, None, 10),\n",
    "                                thr=0); # set thr to 0.1 to see contours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Saving and loading results\n",
    "There is a built-in `save()` method for the `cnmf` object. It can save as `hdf5` or `nwb`.\n",
    "\n",
    "> Note: when you save, you are only saving what is contained in the `cnmf` object. If you want to save other things in your workspace at the same time, you can attach them to your `estimates` object. We'll show how to do this with the correlation image which is useful for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results = True\n",
    "if save_results:\n",
    "    save_path = os.path.join(output_filename, 'ouput.hdf5')\n",
    "    cnmf_refit.estimates.Cn = correlation_image # squirrel away correlation image with cnmf object\n",
    "    cnmf_refit.save(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(intentional error] \n",
    "#if you are editing the code, change this box from raw text to code so that the bits below this don't delete the data you're trying to manipulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading saved results\n",
    "You can use the `load_CNMF()` method to load your saved results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded data.\n"
     ]
    }
   ],
   "source": [
    "load_results = True\n",
    "if load_results:\n",
    "    save_path = os.path.join(output_filename, 'ouput.hdf5')  # or add full/path/to/file.hdf5\n",
    "    cnmf_refit = cnmf.load_CNMF(save_path, \n",
    "                                n_processes=num_processors_to_use, \n",
    "                                dview=cluster)\n",
    "    correlation_image = cnmf_refit.estimates.Cn\n",
    "    print(f\"Successfully loaded data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of Caiman is to help you extract high-quality signals from your data, and we have achieved that goal with the above steps. The next step, of doing actual *analysis* of these results, is where the most interesting neuroscience happens: what are the statistical features of these signals? How do they relate to other environmental, molecular, behavioral, and neuronal features that you are studying? What kinds of visualization and analysis tools can you build around this infrastructure? If you find/publish things that help, please share them with us, as we would love to hear about them! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up open resources\n",
    "We have a few resources we have left open we should take care of.\n",
    "\n",
    "## Shut down cluster \n",
    "To free up processing resources, let's shut down the cluster in case it is still open. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.stop_server(dview=cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shut down logger, optionally remove log files\n",
    "If you set up your logger to log to files, and you don't want to preserve them, you can delete them with the following. If you have custom log filenames, you may have to change the `log_files` pattern for the following to work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shut down logger (otherwise will not be able to delete it)\n",
    "logging.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_logs = True\n",
    "logging_dir = cm.paths.get_tempdir() \n",
    "if delete_logs:\n",
    "    log_files = glob.glob(logging_dir + '\\\\demo_pipeline' + '*' + '.log')\n",
    "    for log_file in log_files:\n",
    "        print(f\"Deleting {log_file}\")\n",
    "        os.remove(log_file)\n",
    "else:\n",
    "    print(f\"If you want to inspect your logs they are in {logging_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
